{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37df2e8",
   "metadata": {},
   "source": [
    "Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbdefda7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_utils_uni'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0_/hc6f6cdn35q2s4ndvcpzmfr00000gn/T/ipykernel_9612/445891541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlab_utils_uni\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplt_house_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplt_contour_wgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplt_divergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplt_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab_utils_uni'"
     ]
    }
   ],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce8a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are loading our data set\n",
    "#we just have 2 data points in this case(1,300 ) and(2,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6461d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array([1.0,2.0]) #features\n",
    "y_train=np.array([300,500])#target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2a91ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate cost\n",
    "#cost _function\n",
    "def compute_cost(x,y,w,b):\n",
    "    m=x.shape[0]\n",
    "    cost=0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb=w*x[i]+b\n",
    "        cost=cost+(f_wb-y[i])**2\n",
    "    total_cost=1/(2*m)*cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce4b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate gradient\n",
    "def compute_gradient(x,y,w,b):\n",
    "    \n",
    "    #number of training samples-m\n",
    "    m=x.shape[0]\n",
    "    dj_dw=0\n",
    "    dj_db=0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb=w*x[i]+b\n",
    "        dj_dw_i=(f_wb - y[i])*x[i]\n",
    "        dj_db_i=(f_wb-y[i])\n",
    "        dj_dw+=dj_dw_i\n",
    "        dj_db+=dj_db_i\n",
    "    dj_dw=dj_dw/m\n",
    "    dj_db=dj_db/m\n",
    "    \n",
    "    return dj_dw,dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ef9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_gradients(x_train,y_train,compute_cost,compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating gradient descent with respect to small change in w and b values\n",
    "\n",
    "J_history=[]\n",
    "p_history=[]  #of w and b\n",
    "b=b_in\n",
    "w=w_in\n",
    "\n",
    "for i in range(num_iters):\n",
    "    dj_dw,dj_db=gradient_function(x,y,w,b)\n",
    "    b=b-alpha*dj_db\n",
    "    w=w-alpha*dj_dw\n",
    "    \n",
    "    #save cost at each iteration\n",
    "    \n",
    "    if i<10000:\n",
    "        J_history.append(cost_function(x,y,w,b))\n",
    "        p_history.append([w,b])\n",
    "        \n",
    "    #print cost\n",
    "    if i%math.ceil(num_iters/10)==0:\n",
    "        print(f\"Iteration {i:4}):Cost {J_history[-1]:0.2e}\",\n",
    "              f\"dj_dw: ____f\"dj_db____f\"w ,b\n",
    "return w,b,J_history,p_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29711d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from iterations , we need final value of w, and b\n",
    "\n",
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cdc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost_vs_gradient descent\n",
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80eeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding optimal values for others using w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469661e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
